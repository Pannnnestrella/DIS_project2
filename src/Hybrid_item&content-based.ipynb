{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid approach of item-based and \"content\"-based methods.\n",
    "For \"content\"-based method, I combined `title`, `subtitle` and `description` to represent the content of the book to compute content_similarity. In regular cases where the user has rated more than 2 times, I adopt item-based method; while in cold start cases where the user has only rated 1 or 2 times, I use the content-based method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load Datasets\n",
    "train = pd.read_csv('/kaggle/input/dis-project-2-recommender-systems-f2024/train.csv')  # book_id, user_id, rating\n",
    "test = pd.read_csv('/kaggle/input/dis-project-2-recommender-systems-f2024/test.csv')    # id, book_id, user_id\n",
    "metadata = pd.read_csv('/kaggle/input/books-metadata/books_augmented.csv')  # book_id, title, description, genre, etc.\n",
    "\n",
    "# Find common book_ids\n",
    "common_book_ids = set(train['book_id']).intersection(metadata['book_id'])\n",
    "\n",
    "# Filter train and metadata datasets\n",
    "train = train[train['book_id'].isin(common_book_ids)]\n",
    "metadata = metadata[metadata['book_id'].isin(common_book_ids)]\n",
    "\n",
    "# Create User-Item Matrix\n",
    "ratings_matrix = train.pivot_table(index='book_id', columns='user_id', values='rating')\n",
    "\n",
    "# Compute Item-Based Similarity\n",
    "# Fill missing values with item mean\n",
    "item_mean_ratings = ratings_matrix.mean(axis=1)\n",
    "ratings_matrix_filled = ratings_matrix.apply(lambda x: x.fillna(item_mean_ratings[x.name]), axis=1)\n",
    "\n",
    "# Compute cosine similarity on the filled matrix\n",
    "item_similarity = cosine_similarity(ratings_matrix_filled)\n",
    "item_similarity_df = pd.DataFrame(item_similarity, index=ratings_matrix.index, columns=ratings_matrix.index)\n",
    "\n",
    "# Compute Content-Based Similarity\n",
    "# Combine metadata into a single textual feature\n",
    "metadata = metadata.drop_duplicates(subset=['book_id'])\n",
    "metadata['description'] = metadata['description'].fillna(\"\")\n",
    "metadata['subtitle'] = metadata['subtitle'].fillna(\"\")\n",
    "metadata['combined_features'] = metadata['title'] + \" \" + metadata['subtitle'] + \" \" + metadata['description']\n",
    "\n",
    "# Use TF-IDF to vectorize combined features\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(metadata['combined_features'])\n",
    "\n",
    "# Compute cosine similarity based on metadata\n",
    "content_similarity = cosine_similarity(tfidf_matrix)\n",
    "content_similarity_df = pd.DataFrame(content_similarity, index=metadata['book_id'], columns=metadata['book_id'])\n",
    "\n",
    "# Find common book_ids between the two similarity matrices\n",
    "common_book_ids = item_similarity_df.index.intersection(content_similarity_df.index)\n",
    "\n",
    "# Align both similarity matrices\n",
    "item_similarity_df = item_similarity_df.loc[common_book_ids, common_book_ids]\n",
    "content_similarity_df = content_similarity_df.loc[common_book_ids, common_book_ids]\n",
    "\n",
    "# Define Adaptive Prediction Function\n",
    "book_rating_counts = train['book_id'].value_counts()\n",
    "min_ratings = 3  # Threshold for cold start\n",
    "\n",
    "def predict_rating(user_id, book_id, ratings_matrix, item_similarity_df, content_similarity_df, item_mean_ratings):\n",
    "    # Check if book_id exists in the similarity matrix\n",
    "    if book_id not in item_similarity_df.index:\n",
    "        return item_mean_ratings.mean()  # Fallback to global mean if book_id is unknown\n",
    "\n",
    "    # Determine if the book is a cold-start case\n",
    "    is_cold_start = book_rating_counts.get(book_id, 0) < min_ratings\n",
    "\n",
    "    if is_cold_start:\n",
    "        # Use content-based similarity exclusively\n",
    "        if user_id in ratings_matrix.columns:\n",
    "            user_ratings = ratings_matrix.loc[:, user_id]\n",
    "            rated_books = user_ratings[user_ratings > 0].index  # Books the user rated\n",
    "        else:\n",
    "            return item_mean_ratings.mean()  # Fallback for unknown users\n",
    "\n",
    "        rated_books = [b for b in rated_books if b in content_similarity_df.index]\n",
    "        if len(rated_books) == 0:\n",
    "            return item_mean_ratings.mean()  # Fallback if no valid rated books\n",
    "\n",
    "        similarities = content_similarity_df.loc[book_id, rated_books]\n",
    "        ratings = user_ratings[rated_books]\n",
    "\n",
    "        # Compute weighted average\n",
    "        weighted_sum = np.dot(similarities, ratings)\n",
    "        sim_sum = np.abs(similarities).sum()\n",
    "\n",
    "        return weighted_sum / sim_sum if sim_sum > 0 else item_mean_ratings.mean()\n",
    "\n",
    "    else:\n",
    "        # Use collaborative or blended similarity\n",
    "        if user_id in ratings_matrix.columns:\n",
    "            user_ratings = ratings_matrix.loc[:, user_id]\n",
    "            rated_books = user_ratings[user_ratings > 0].index  # Books the user rated\n",
    "        else:\n",
    "            return item_mean_ratings.mean()  # Fallback for unknown users\n",
    "\n",
    "        rated_books = [b for b in rated_books if b in item_similarity_df.index]\n",
    "        if len(rated_books) == 0:\n",
    "            return item_mean_ratings.mean()  # Fallback if no valid rated books\n",
    "\n",
    "        similarities = item_similarity_df.loc[book_id, rated_books]\n",
    "        ratings = user_ratings[rated_books]\n",
    "\n",
    "        # Compute weighted average\n",
    "        weighted_sum = np.dot(similarities, ratings)\n",
    "        sim_sum = np.abs(similarities).sum()\n",
    "\n",
    "        return weighted_sum / sim_sum if sim_sum > 0 else item_mean_ratings.mean()\n",
    "\n",
    "# Generate Predictions for Test Set\n",
    "predictions = []\n",
    "for _, row in test.iterrows():\n",
    "    book_id = row['book_id']\n",
    "    user_id = row['user_id']\n",
    "    pred_rating = predict_rating(user_id, book_id, ratings_matrix_filled, item_similarity_df, content_similarity_df, item_mean_ratings)\n",
    "    predictions.append({'id': row['id'], 'rating': pred_rating})\n",
    "\n",
    "# Save Predictions\n",
    "submission = pd.DataFrame(predictions)\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9915979,
     "sourceId": 87197,
     "sourceType": "competition"
    },
    {
     "datasetId": 6220786,
     "sourceId": 10088863,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
